<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lesson 5 - Reliability and Agreement &#8212; HEADS ANALYSE v0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=4f649999" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=039e1c02" />
    <script src="_static/documentation_options.js?v=34cd777e"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lesson 6 - Chi-squared test; Simple Linear Regression" href="lesson_06.html" />
    <link rel="prev" title="Lesson 4 - Hypothesis testing; Parametric tests; Nonparametric testing" href="lesson_04.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="lesson-5-reliability-and-agreement">
<h1>Lesson 5 - Reliability and Agreement<a class="headerlink" href="#lesson-5-reliability-and-agreement" title="Link to this heading">¶</a></h1>
<p><strong>Lecturer</strong>: Professora Cristina Santos</p>
<p><strong>Topic</strong>: Reliability and Agreement</p>
<section id="context">
<h2>Context<a class="headerlink" href="#context" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>The quality of the measurements taken by health professionals or by measurement devices is fundamental  not only for clinical care but also for  research</p></li>
<li><p>Measurement of variables always implies some degree of error.</p></li>
<li><p>When an observer takes a measurement, thevalue obtained depends on several things such us:</p>
<ul>
<li><p>the skills of the observer,</p></li>
<li><p>observer experience,</p></li>
<li><p>the measurement instrument,</p></li>
<li><p>observer’s expectations</p></li>
<li><p>…</p></li>
</ul>
</li>
<li><p>Also, natural continuous variation in a biological quantity can be present.</p></li>
<li><p>When natural continuous variation in a  biological quantity is present, it is outside the control of the observer.</p></li>
<li><p>It is, however, possible to minimize the observer variability by:</p>
<ul>
<li><p>training of observers,</p></li>
<li><p>use of guidelines</p></li>
<li><p>automation, …</p></li>
</ul>
</li>
<li><p>Reliability and agreement studies before Validity studies / RCT / …</p></li>
<li><p>Agreement studies are very importante however neglected in medical literature</p></li>
</ul>
</section>
<section id="reliability-versus-agreement">
<h2>Reliability versus Agreement<a class="headerlink" href="#reliability-versus-agreement" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>The terms “reliability” and “agreement” are often used interchangeably.</p></li>
<li><p>However, the two concepts are conceptually distinct</p></li>
<li><p><strong>Reliability</strong> can be defined as the ability of a measurement to differentiate between subjects.</p>
<ul>
<li><p>Reliability may be defined as the ratio of variability between subjects (e.g., patients) or objects (e.g., computed tomography scans) to the total variability of all measurements in the sample</p></li>
</ul>
</li>
<li><p><strong>Agreement</strong> is the degree to which scores or ratings are identical.</p></li>
<li><p>Both concepts are important, because they provide information about the quality of measurements.</p></li>
<li><p>The study designs for examining the two concepts are similar.</p></li>
<li><p>We focus on two aspects of these concepts:</p>
<ul>
<li><p><strong>Interrater agreement/reliability</strong> (different raters, using the same scale, classification, instrument, or procedure, assess the same subjects or objects).</p></li>
<li><p><strong>Intrarater agreement/reliability</strong> (also referred to as test–retest) (the same rater, using the same scale, classification, instrument or procedure, assesses the same subjects or object at different times).</p></li>
</ul>
</li>
</ul>
<p><img alt="Alt text" src="_images/image5.1.png" /></p>
</section>
<section id="reliability-measures">
<h2>Reliability measures<a class="headerlink" href="#reliability-measures" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Nominal:</p>
<ul>
<li><p>Kappa statistics</p></li>
</ul>
</li>
<li><p>Ordinal:</p>
<ul>
<li><p>Ranked intraclass correlation</p></li>
<li><p>Matrix of kappa coefficients</p></li>
<li><p>Weighted kappa</p></li>
</ul>
</li>
<li><p>Continuous:xzw</p>
<ul>
<li><p>Intraclass correlation coefficients</p></li>
</ul>
</li>
</ul>
</section>
<section id="agreement-measures">
<h2>Agreement measures<a class="headerlink" href="#agreement-measures" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Nominal and ordinal:</p>
<ul>
<li><p>Proportions of agreement</p></li>
<li><p>Proportions of specific agreement</p></li>
</ul>
</li>
<li><p>Continuous:</p>
<ul>
<li><p>Proportions of agreement (ranges)</p></li>
<li><p>Proportions of specific agreement (ranges)</p></li>
<li><p>Standard errors of measurement</p></li>
<li><p>Coefficients of variation</p></li>
<li><p>Brand-Altman plots and limits of agreement</p></li>
</ul>
</li>
</ul>
</section>
<section id="cohen-s-kappa-coefficient">
<h2>Cohen’s Kappa coefficient<a class="headerlink" href="#cohen-s-kappa-coefficient" title="Link to this heading">¶</a></h2>
<p>Two raters are asked to classify objects into categories 1 and 2.
The table below contains cell probabilities for a 2 by 2 table.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head text-center"><p>Rater #1</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>2</p></td>
<td class="text-center"><p>Total</p></td>
</tr>
<tr class="row-odd"><td><p>Rater #2</p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>p<sub>11</sub></p></td>
<td class="text-center"><p>p<sub>12</sub></p></td>
<td class="text-center"><p>p<sub>1.</sub></p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>2</p></td>
<td class="text-center"><p>p<sub>21</sub></p></td>
<td class="text-center"><p>p<sub>22</sub></p></td>
<td class="text-center"><p>p<sub>2.</sub></p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td class="text-center"><p>Total</p></td>
<td class="text-center"><p>p<sub>.1</sub></p></td>
<td class="text-center"><p>p<sub>.2</sub></p></td>
<td class="text-center"><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>Proportions of agreement</strong> (P<sub>o</sub>) = P<sub>11</sub> + P<sub>22</sub></p>
<p><strong>Proportions of expected by chance</strong> (P<sub>e</sub>) = P<sub>.1</sub> P<sub>1.</sub> + P<sub>.2</sub> P<sub>2.</sub></p>
<p>K = (P<sub>o</sub> - P<sub>e</sub>) / (1 - P<sub>e</sub>)</p>
<ul class="simple">
<li><p>Chance-Corrected Agreement? Or measure of reliability?</p></li>
</ul>
<section id="high-agreement-but-low-kappa">
<h3>High agreement but low kappa<a class="headerlink" href="#high-agreement-but-low-kappa" title="Link to this heading">¶</a></h3>
<p>Example:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Observer 2</p></th>
<th class="head text-center"><p>Observer 1</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>yes</p></td>
<td class="text-center"><p>no</p></td>
<td class="text-center"><p>total</p></td>
</tr>
<tr class="row-odd"><td><p>yes</p></td>
<td class="text-center"><p>40</p></td>
<td class="text-center"><p>5</p></td>
<td class="text-center"><p>45</p></td>
</tr>
<tr class="row-even"><td><p>no</p></td>
<td class="text-center"><p>3</p></td>
<td class="text-center"><p>2</p></td>
<td class="text-center"><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>total</p></td>
<td class="text-center"><p>43</p></td>
<td class="text-center"><p>7</p></td>
<td class="text-center"><p>50</p></td>
</tr>
</tbody>
</table>
<p>PA=42/50=0.84</p>
<p>Pe=(43/50)(45/50)+(7/50)(5/50)=0.79</p>
<p>K=(0.84-0.79)/(1-0.79)=0.24</p>
<section id="specific-agreement">
<h4>Specific agreement<a class="headerlink" href="#specific-agreement" title="Link to this heading">¶</a></h4>
<p><strong>Summary of binary ratings by two raters</strong>:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Rater 1</p></th>
<th class="head text-center"><p>Rater 2 +</p></th>
<th class="head text-center"><p>Rater 2 -</p></th>
<th class="head text-center"><p>total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>+</p></td>
<td class="text-center"><p>a</p></td>
<td class="text-center"><p>b</p></td>
<td class="text-center"><p>a + b</p></td>
</tr>
<tr class="row-odd"><td><p>-</p></td>
<td class="text-center"><p>c</p></td>
<td class="text-center"><p>d</p></td>
<td class="text-center"><p>c + d</p></td>
</tr>
<tr class="row-even"><td><p>total</p></td>
<td class="text-center"><p>a + c</p></td>
<td class="text-center"><p>b + d</p></td>
<td class="text-center"><p>N</p></td>
</tr>
</tbody>
</table>
<p>The values a, b, c and d here denote the observed frequencies for each possible combination of ratings by Rater 1 and Rater 2.</p>
<p>Proportion of overall agreement = (a+d)/N</p>
<p><strong>Positive Agreement</strong> estimates the conditional probability, given that one of the raters, randomly selected, makes a positive rating, the other rater will also do so</p>
<p>PA = (\frac{2a}{2a + b + c})</p>
<p><strong>Negative Agreement</strong> estimates the conditional probability, given that one of the raters, randomly selected, makes a negative rating, the other rater will also do so</p>
<p>NA = (\frac{2d}{2d + b + c})</p>
<p><strong>Application on the example:</strong>
PA  yes = 40x2/(40x2+3+5)=0.92
PA no = 2x2/(2x2+3+5)=0.33</p>
</section>
</section>
</section>
<section id="intraclass-correlation-coefficient">
<h2>Intraclass Correlation Coefficient<a class="headerlink" href="#intraclass-correlation-coefficient" title="Link to this heading">¶</a></h2>
<p>The Intraclass Correlation (ICC) assesses rating reliability by comparing the variability of different ratings of the same subject to the total variation across all ratings and all subjects.</p>
</section>
<section id="information-based-measure-of-disagreement">
<h2>Information Based Measure of Disagreement<a class="headerlink" href="#information-based-measure-of-disagreement" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>The sum over all logarithms of possible outcomes of the variable is a valid measure of the amount of information, or uncertainty, contained in a variable.</p>
</div></blockquote>
<p><em>Costa-Santos C. et al. Assessment of Disagreement: A New Information-Based Approach. ANNALS OF EPIDEMIOLOGY. 20(7):555-561</em></p>
<p>Consider that we aim to measure disagreement between measurements obtained by Observer Y and Observer X. The disagreement between Y and X is related to the differences between them. So, we consider</p>
<p><img alt="Alt text" src="_images/image5.2.png" /></p>
<p>the amount of information contained in the differences between observers.</p>
<p>By adding 1 to the differences, we avoid the behavior of the logarithmic function between 0 and 1.
To get a value between 0 and 1 we normalize the amount of information contained in the differences to  obtain the following measure of information-based measure of disagreement (IBMD):</p>
<p><img alt="Alt text" src="_images/image5.3.png" /></p>
<p>0 – no disagreement
Tends to 1 (total disagreement)</p>
</section>
<section id="bland-and-altman-limits-of-agreement">
<h2>Bland and Altman limits of agreement<a class="headerlink" href="#bland-and-altman-limits-of-agreement" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>Difference against mean</p>
</div></blockquote>
<ul class="simple">
<li><p>Provided differences within mean differences +/- 2SD would not be clinically important, we could use  the two measurement methods interchangeably.</p></li>
<li><p>It is a clinical (not statistical!) interpretation.</p></li>
</ul>
</section>
<section id="summary-of-interpretations">
<h2>Summary of interpretations<a class="headerlink" href="#summary-of-interpretations" title="Link to this heading">¶</a></h2>
<section id="agreement">
<h3>Agreement<a class="headerlink" href="#agreement" title="Link to this heading">¶</a></h3>
<section id="proportion-of-agreement-pa">
<h4>Proportion of agreement (PA)<a class="headerlink" href="#proportion-of-agreement-pa" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><p>If one observer, selected at random, makes an observation (may be further specified), the probability of another observer making an equal obversation is 0.XXX</p></li>
</ul>
</section>
<section id="proportion-of-specific-agreement-positive-agreement-pa-negative-agreement-na-etc">
<h4>Proportion of Specific Agreement (Positive Agreement - PA; Negative Agreement - NA; etc)<a class="headerlink" href="#proportion-of-specific-agreement-positive-agreement-pa-negative-agreement-na-etc" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><p>If one observer, selected at random, makes the observation X, the probability of another observer making the same obversation is 0.XXX</p></li>
<li><p>If one observer, selected at random, makes the observation Y, the probability of another observer making the same obversation is 0.YYY</p></li>
<li><p>…</p></li>
</ul>
</section>
</section>
<section id="reliability">
<h3>Reliability<a class="headerlink" href="#reliability" title="Link to this heading">¶</a></h3>
<section id="k-statistic">
<h4>K statistic<a class="headerlink" href="#k-statistic" title="Link to this heading">¶</a></h4>
<p>K = 1 implies perfect “agreement” and K = 0 suggests that the “agreement” is no better than that which would be obtained by chance.
There are no objective criteria for judging intermediate values. However, kappa is often judged as providing “agreement” which is:</p>
<ul class="simple">
<li><p>poor if KI 0.20;</p></li>
<li><p>fair if 0.21 2 K&lt; 0.40;</p></li>
<li><p>moderate if 0.41 I K&lt; 0.60;</p></li>
<li><p>substantial if 0.61 I KI 0.80;</p></li>
<li><p>good if K&gt; 0.80.</p></li>
</ul>
</section>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">HEADS ANALYSE</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lesson_01.html">Lesson 1 - Data Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_02.html">Lesson 2 - Descriptive Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_03.html">Lesson 3 - Probability Distributions; Sampling and estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_04.html">Lesson 4 - Hypothesis testing; Parametric tests; Nonparametric testing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lesson 5 - Reliability and Agreement</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#context">Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reliability-versus-agreement">Reliability versus Agreement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reliability-measures">Reliability measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#agreement-measures">Agreement measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cohen-s-kappa-coefficient">Cohen’s Kappa coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#intraclass-correlation-coefficient">Intraclass Correlation Coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#information-based-measure-of-disagreement">Information Based Measure of Disagreement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bland-and-altman-limits-of-agreement">Bland and Altman limits of agreement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary-of-interpretations">Summary of interpretations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lesson_06.html">Lesson 6 - Chi-squared test; Simple Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_06.html#chi-squared-test">Chi-squared test</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_06.html#statistical-methods">Statistical Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_06.html#regression">Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_06.html#simple-linear-regression">Simple Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_07.html">Lesson 7 - Multiple Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_08.html">Lesson 8 - Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_09.html">Lesson 9 - Machine Learning and Data Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="lesson_10.html">Lesson 9 - Learning and evaluating classifiers</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="lesson_04.html" title="previous chapter">Lesson 4 - Hypothesis testing; Parametric tests; Nonparametric testing</a></li>
      <li>Next: <a href="lesson_06.html" title="next chapter">Lesson 6 - Chi-squared test; Simple Linear Regression</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2024, Mariana Canelas-Pais.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/lesson_05.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>